{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import xml.etree.ElementTree as ex\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "sample_labels = [\"LMCI\", \"AD\"]\n",
    "label_to_id = {}      # key: class name\n",
    "                    # value: class id\n",
    "file_to_apoe = {}   # key: image file path\n",
    "                    # value: boolean, true if image specified 4 in either \"APOE A1\" or \"APOE A2\" fields\n",
    "\n",
    "# This is for getting ADNI files\n",
    "def get_filenames(folder, metadata_folder):\n",
    "    global label_to_id\n",
    "    for i, label in enumerate(sample_labels):\n",
    "        label_to_id[label] = i\n",
    "         \n",
    "    # Get list of images\n",
    "    image_class = {} # key: unique subject id in filename\n",
    "                     # value: image file path\n",
    "    for root, directories, filenames in os.walk(folder):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\"nii\") and \"__Scaled_2\" not in root:\n",
    "                identifier = filename[-11:-4]\n",
    "                image_class[identifier] = os.path.join(root,filename)\n",
    "                \n",
    "    meta_files = [f for f in os.listdir(metadata_folder) if f.endswith('xml')]\n",
    "        \n",
    "    # Label each image\n",
    "    classify_by_label = {} # key: label class in [0, 1, 2, 3]\n",
    "                           # value: list of image file paths\n",
    "    for xml in meta_files:\n",
    "        if \"__Scaled_2\" not in xml:\n",
    "            identifier = xml[-11:-4]\n",
    "            root = ex.parse(metadata_folder + \"/\" + xml).getroot()\n",
    "            classification = root[0][3][1].text\n",
    "            if classification in label_to_id:\n",
    "                cl = label_to_id[classification]\n",
    "                file_path = image_class[identifier]\n",
    "                if cl not in classify_by_label:\n",
    "                    classify_by_label[cl] = [file_path]\n",
    "                else:\n",
    "                    classify_by_label[cl].append(file_path)\n",
    "                \n",
    "                # Storing APOE e4 carrier information for each image file\n",
    "                apoe_1 = root[0][3][4].text\n",
    "                apoe_2 = root[0][3][5].text\n",
    "                \n",
    "                file_to_apoe[file_path] = False\n",
    "                if apoe_1 == \"4\" or apoe_2 == \"4\":\n",
    "                    file_to_apoe[file_path] = True\n",
    "                    \n",
    "    print(\"Statistics: {}\".format(get_stats(classify_by_label)))\n",
    "    \n",
    "    return classify_by_label\n",
    "\n",
    "def get_stats(files):\n",
    "    stats = {}\n",
    "    for k, v in files.items():\n",
    "        stats[sample_labels[int(k)]] = len(v)\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nilearn import image\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy import ndimage as nd\n",
    "\n",
    "def get_files(data_folder, metadata_folder):\n",
    "    files = get_filenames(data_folder, metadata_folder)\n",
    "    return files \n",
    "\n",
    "# Size of desired image\n",
    "width = 80\n",
    "height = 80\n",
    "depth = 80\n",
    "\n",
    "def get_data(files, class_id):\n",
    "    global width, height, depth\n",
    "    num_images = len(files)\n",
    "    \n",
    "    x_data = np.zeros([num_images, height, width, depth], np.float32)\n",
    "    y_data = np.zeros(num_images, dtype=np.int32)\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        loaded_img = nib.load(files[i])    \n",
    "        im = loaded_img.get_data()\n",
    "        im = nd.interpolation.zoom(im, \n",
    "                                   zoom = np.array([height, width, depth])/im.shape)\n",
    "\n",
    "        x_data[i] = np.asarray(im, dtype=np.float32)\n",
    "        y_data[i] = class_id\n",
    "    \n",
    "    x_data_ = x_data.reshape(num_images, height * width * depth)\n",
    "    return x_data_, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data(num_result, class_data, class_id):\n",
    "    random_inds = random.sample(range(0, len(class_data)), num_result)\n",
    "    random_inds = sorted(random_inds, reverse=True)\n",
    "    files = []\n",
    "    for ind in random_inds:\n",
    "        files.append(class_data.pop(ind))\n",
    "    data, target = get_data(files, class_id)\n",
    "    \n",
    "    return data, target, files\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "# Get training data\n",
    "files = get_files('ADNI2_all/ADNI', 'ADNI2_all/metadata')\n",
    "num_samples = 0\n",
    "flattened = [[] for i in range(len(sample_labels))]\n",
    "for k, v in files.items():\n",
    "    for f in v:\n",
    "        flattened[k].append(f) \n",
    "        num_samples += 1\n",
    "\n",
    "# Store APOE e4 carrier statistics on test data\n",
    "apoe_test_files = []\n",
    "e4_carrier_stats_pre = {}\n",
    "for i in range(0, len(sample_labels)):\n",
    "    e4_carrier_stats_pre[i] = 0\n",
    "    \n",
    "num_val_samples_per_cl = 8\n",
    "num_test_samples_per_cl = 16\n",
    "\n",
    "# Split training and testing data\n",
    "tr_data = []\n",
    "tr_target = []\n",
    "val_data = []\n",
    "val_target = []\n",
    "test_data = []\n",
    "test_target = []\n",
    "for cl in range(0, len(flattened)): \n",
    "    # Get testing data\n",
    "    data, target, files = partition_data(num_test_samples_per_cl, flattened[cl], cl)\n",
    "    if len(test_data) == 0:\n",
    "        test_data = data\n",
    "        test_target = target\n",
    "    else:\n",
    "        test_data = np.append(test_data, data, axis=0)\n",
    "        test_target = np.append(test_target, target)\n",
    "                \n",
    "    # Increment values if e4 carrier status\n",
    "    apoe_test_files = np.append(apoe_test_files, files)\n",
    "    for f in files:\n",
    "        if file_to_apoe[f]:\n",
    "            e4_carrier_stats_pre[cl] += 1\n",
    "    \n",
    "    # Get validation data\n",
    "    data, target, files = partition_data(num_val_samples_per_cl, flattened[cl], cl)\n",
    "    if len(val_data) == 0:\n",
    "        val_data = data\n",
    "        val_target = target\n",
    "    else:\n",
    "        val_data = np.append(val_data, data, axis=0)\n",
    "        val_target = np.append(val_target, target)\n",
    "    \n",
    "    # Get training data\n",
    "    data, target = get_data(flattened[cl], cl)\n",
    "    if len(tr_data) == 0:\n",
    "        tr_data = data\n",
    "        tr_target = target\n",
    "    else:\n",
    "        tr_data = np.append(tr_data, data, axis=0)\n",
    "        tr_target = np.append(tr_target, target)\n",
    "    \n",
    "# Print e4 carrier stats\n",
    "for cl_id, quant in e4_carrier_stats_pre.items():\n",
    "    print(\"Class {}. Pct APOE e4 carrier {}\".format(sample_labels[cl_id], quant/num_test_samples_per_cl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv3d(x, W, bias, strides=[1, 1, 1, 1, 1]):\n",
    "    conv = tf.nn.conv3d(x, W, strides, padding='SAME') \n",
    "    bias = tf.nn.bias_add(conv, bias)\n",
    "    return tf.nn.relu(bias)\n",
    "\n",
    "def batch_norm(x, training):\n",
    "    return tf.layers.batch_normalization(x, training = training)\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool3d(x, ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(shape = [None, width*height*depth],\n",
    "                   dtype = tf.float32)\n",
    "training = tf.placeholder(shape = None,\n",
    "                          dtype = tf.bool)\n",
    "\n",
    "x_image = tf.reshape(x, [-1, height, width, depth, 1])\n",
    "\n",
    "# Block 1\n",
    "W = weight([3, 3, 3, 1, 8])\n",
    "b = bias([8])\n",
    "conv1 = conv3d(x_image, W, b)\n",
    "\n",
    "W = weight([3, 3, 3, 8, 8])\n",
    "conv2 = conv3d(conv1, W, b)\n",
    "pool = max_pool_2x2(conv2)\n",
    "\n",
    "# Block 2\n",
    "W = weight([3, 3, 3, 8, 16])\n",
    "b = bias([16])\n",
    "conv3 = conv3d(pool, W, b)\n",
    "\n",
    "W = weight([3, 3, 3, 16, 16])\n",
    "conv4 = conv3d(conv3, W, b)\n",
    "pool = max_pool_2x2(conv4)\n",
    "\n",
    "# Block 3\n",
    "W = weight([3, 3, 3, 16, 32])\n",
    "b = bias([32])\n",
    "conv5 = conv3d(pool, W, b)\n",
    "\n",
    "W = weight([3, 3, 3, 32, 32])\n",
    "conv6 = conv3d(conv5, W, b)\n",
    "conv7 = conv3d(conv6, W, b)\n",
    "conv8 = conv3d(conv7, W, b)\n",
    "pool = max_pool_2x2(conv8)\n",
    "\n",
    "# Block 4\n",
    "W = weight([3, 3, 3, 32, 64])\n",
    "b = bias([64])\n",
    "conv9 = conv3d(pool, W, b)\n",
    "\n",
    "W = weight([3, 3, 3, 64, 64])\n",
    "conv10 = conv3d(conv9, W, b)\n",
    "conv11 = conv3d(conv10, W, b)\n",
    "conv12 = conv3d(conv11, W, b)\n",
    "pool = max_pool_2x2(conv12)\n",
    "\n",
    "# Block 5\n",
    "W = weight([3, 3, 3, 64, 128])\n",
    "b = bias([128])\n",
    "conv13 = conv3d(pool, W, b)\n",
    "\n",
    "W = weight([3, 3, 3, 128, 128])\n",
    "conv14 = conv3d(conv13, W, b)\n",
    "conv15 = conv3d(conv14, W, b)\n",
    "conv16 = conv3d(conv15, W, b)\n",
    "pool = max_pool_2x2(conv16)\n",
    "\n",
    "# Densely Connected Layer (or fully-connected layer)\n",
    "pool_flat = tf.layers.flatten(pool)\n",
    "fcl17 = tf.layers.dense(pool_flat, units=128, activation = tf.nn.relu)\n",
    "bn = batch_norm(fcl17, training)\n",
    "fcl18 = tf.layers.dense(bn, units=64, activation = tf.nn.relu)\n",
    "logits = tf.layers.dense(fcl18,\n",
    "                         units = len(sample_labels),\n",
    "                         activation = tf.nn.softmax)\n",
    "\n",
    "# Loss\n",
    "y_ = tf.placeholder(shape = [None],\n",
    "                    dtype = tf.int32)\n",
    "y_onehot = tf.one_hot(y_, len(sample_labels))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_onehot, \n",
    "                                                                 logits=logits))\n",
    "\n",
    "# Classification Accuracy\n",
    "predicted = tf.argmax(logits, 1)\n",
    "correct_prediction = tf.equal(predicted, tf.argmax(y_onehot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Optimizer\n",
    "opt = tf.train.AdamOptimizer(5e-6).minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting variables\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_val = []\n",
    "y_val = []\n",
    "x_tr_acc = []\n",
    "y_tr_acc = []\n",
    "x_val_acc = []\n",
    "y_val_acc = []\n",
    "\n",
    "def plotting(train_loss, val_loss, train_acc, val_acc, final=False):\n",
    "    if final:\n",
    "        plt.figure(figsize=(20,10))\n",
    "    plt.scatter(train_loss[0], train_loss[1], color='g', s=1, label='training')\n",
    "    plt.scatter(val_loss[0], val_loss[1], color='r', s=1, label='validation')\n",
    "    plt.xlabel('Epochs', fontsize=16)\n",
    "    plt.ylabel('Loss', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    if final:\n",
    "        plt.figure(figsize=(20,10))\n",
    "    plt.scatter(train_acc[0], train_acc[1], color='g', s=1, label='training')\n",
    "    plt.scatter(val_acc[0], val_acc[1], color='r', s=1, label='validation')\n",
    "    plt.xlabel('Epochs', fontsize=16)\n",
    "    plt.ylabel('Accuracy', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Params for early stopping\n",
    "patience = 16\n",
    "patience_cnt = 0\n",
    "hist_len = 50\n",
    "last_avg_loss = 0\n",
    "hist_loss = []\n",
    "min_loss_seen = None\n",
    "\n",
    "batch_size = 32\n",
    "epoch = 0\n",
    "while (patience_cnt < patience or epoch < hist_len) and epoch < 500:\n",
    "    c = list(zip(tr_data, tr_target))\n",
    "    random.shuffle(c)\n",
    "    tr_data, tr_target = zip(*c)\n",
    "\n",
    "    tr_loss, val_loss, tr_acc, val_acc = (0, 0, 0, 0)\n",
    "    for j in range(0, len(tr_target), batch_size):\n",
    "\n",
    "        # Validation conditions\n",
    "        if j == 0:\n",
    "            # Get accuracy and loss from validation data\n",
    "            vLoss, vAcc, vPred = sess.run([loss, accuracy, predicted], \n",
    "                                          feed_dict = {x: val_data,\n",
    "                                                       y_: val_target,\n",
    "                                                       training: False})\n",
    "                   \n",
    "            print(vPred)\n",
    "            \n",
    "            # Conditions for early stopping\n",
    "            if len(hist_loss) == hist_len:\n",
    "                hist_loss.pop(0)\n",
    "            hist_loss.append(vLoss)\n",
    "            curr_loss = np.mean(np.asarray(hist_loss)) \n",
    "            \n",
    "            print(\"Last loss {}. Curr loss {}.\".format(last_avg_loss, curr_loss))\n",
    "            \n",
    "            if last_avg_loss > curr_loss:\n",
    "                patience_cnt = 0\n",
    "                \n",
    "                # Save the model \n",
    "                if min_loss_seen == None or min_loss_seen > curr_loss:\n",
    "                    min_loss_seen = curr_loss\n",
    "                    saver.save(sess, './braincnn-19-model', global_step=epoch)\n",
    "                    saver.save(sess, './braincnn-19-model-final')\n",
    "            elif last_avg_loss == curr_loss:\n",
    "                patience_cnt = 0\n",
    "            else:\n",
    "                if epoch >= hist_len:\n",
    "                    patience_cnt += 1\n",
    "                \n",
    "            last_avg_loss = curr_loss\n",
    "                \n",
    "        # Training\n",
    "        rOpt, rLoss, rAcc = sess.run([opt, loss, accuracy], \n",
    "                                     feed_dict = {x: tr_data[j:j + batch_size],\n",
    "                                                  y_: tr_target[j:j + batch_size],\n",
    "                                                  training: True})\n",
    "\n",
    "        # Accumulate loss and accuracy stats to later calculate average of epoch\n",
    "        tr_loss += rLoss\n",
    "        val_loss += vLoss\n",
    "        tr_acc += rAcc\n",
    "        val_acc += vAcc\n",
    "\n",
    "        if j == 0:\n",
    "            print('Epoch {}. TrainL {}. ValL {}. Acc {}.'.format(epoch, rLoss, vLoss, vAcc))\n",
    "    \n",
    "    num_mini_batches = math.ceil(len(tr_target)/batch_size)\n",
    "    x_train.append(epoch)\n",
    "    y_train.append(tr_loss/num_mini_batches)\n",
    "    x_val.append(epoch)\n",
    "    y_val.append(val_loss/num_mini_batches)\n",
    "    x_tr_acc.append(epoch)\n",
    "    y_tr_acc.append(tr_acc/num_mini_batches)\n",
    "    x_val_acc.append(epoch)\n",
    "    y_val_acc.append(val_acc/num_mini_batches)\n",
    "    \n",
    "    # Plotting every 20 epochs\n",
    "    if epoch % 20 == 0:\n",
    "        plotting((x_train, y_train), (x_val, y_val), (x_tr_acc, y_tr_acc), (x_val_acc, y_val_acc))\n",
    "    \n",
    "    # Next epoch\n",
    "    epoch += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot final graph\n",
    "plotting((x_train, y_train), (x_val, y_val), (x_tr_acc, y_tr_acc), (x_val_acc, y_val_acc), final=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to restore model saved by early stopping\n",
    "with tf.Session() as sSess:\n",
    "    # Restore variables.\n",
    "    saver.restore(sSess, './braincnn-19-model-final')\n",
    "    print(\"Model restored.\")\n",
    "              \n",
    "    # Get accuracy and loss from validation data\n",
    "    sAcc, sPred = sSess.run([accuracy, predicted], \n",
    "                            feed_dict = {x: test_data,\n",
    "                                         y_: test_target,\n",
    "                                         training: False})\n",
    "    \n",
    "    print('Predicted classes {}'.format(sPred))\n",
    "    print('Testing accuracy {}'.format(sAcc))\n",
    "\n",
    "    # APOE e4 stats for testing data\n",
    "    e4_carrier_stats_post = {}\n",
    "    for k in range(0, len(sample_labels)):\n",
    "        e4_carrier_stats_post[k] = [0, 0]\n",
    "\n",
    "    # Get testing APOE e4 stats\n",
    "    for p in range(0, len(sPred)):\n",
    "        e4_carrier_stats_post[sPred[p]][1] += 1\n",
    "        if file_to_apoe[apoe_test_files[p]]:\n",
    "            e4_carrier_stats_post[sPred[p]][0] += 1\n",
    "\n",
    "    for cl_id, [quant, num_samples] in e4_carrier_stats_post.items():\n",
    "        if num_samples != 0:\n",
    "            print(\"Class {}. Pct APOE e4 carrier {}\".format(sample_labels[cl_id], \n",
    "                                                            quant/num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
